import pandas as pd
import numpy as np
df = pd.read_csv('https://github.com/Dharmesh0073/Dataset/raw/main/Hill%20Valley%20Dataset.csv')
df.head()
V1	V2	V3	V4	V5	V6	V7	V8	V9	V10	...	V92	V93	V94	V95	V96	V97	V98	V99	V100	Class
0	39.02	36.49	38.20	38.85	39.38	39.74	37.02	39.53	38.81	38.79	...	36.62	36.92	38.80	38.52	38.07	36.73	39.46	37.50	39.10	0
1	1.83	1.71	1.77	1.77	1.68	1.78	1.80	1.70	1.75	1.78	...	1.80	1.79	1.77	1.74	1.74	1.80	1.78	1.75	1.69	1
2	68177.69	66138.42	72981.88	74304.33	67549.66	69367.34	69169.41	73268.61	74465.84	72503.37	...	73438.88	71053.35	71112.62	74916.48	72571.58	66348.97	71063.72	67404.27	74920.24	1
3	44889.06	39191.86	40728.46	38576.36	45876.06	47034.00	46611.43	37668.32	40980.89	38466.15	...	42625.67	40684.20	46960.73	44546.80	45410.53	47139.44	43095.68	40888.34	39615.19	0
4	5.70	5.40	5.28	5.38	5.27	5.61	6.00	5.38	5.34	5.87	...	5.17	5.67	5.60	5.94	5.73	5.22	5.30	5.73	5.91	0
5 rows × 101 columns

df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 1212 entries, 0 to 1211
Columns: 101 entries, V1 to Class
dtypes: float64(100), int64(1)
memory usage: 956.5 KB
df.describe()
V1	V2	V3	V4	V5	V6	V7	V8	V9	V10	...	V92	V93	V94	V95	V96	V97	V98	V99	V100	Class
count	1212.000000	1212.000000	1212.000000	1212.000000	1212.000000	1212.000000	1212.000000	1212.000000	1212.000000	1212.000000	...	1212.000000	1212.000000	1212.000000	1212.000000	1212.000000	1212.000000	1212.000000	1212.000000	1212.000000	1212.000000
mean	8169.091881	8144.306262	8192.653738	8176.868738	8128.297211	8173.030008	8188.582748	8183.641543	8154.670066	8120.767574	...	8120.056815	8125.917409	8158.793812	8140.885421	8213.480611	8185.594002	8140.195355	8192.960891	8156.197376	0.500000
std	17974.950461	17881.049734	18087.938901	17991.903982	17846.757963	17927.114105	18029.562695	18048.582159	17982.390713	17900.798206	...	17773.190621	17758.182403	17919.510371	17817.945646	18016.445265	17956.084223	17768.356106	18064.781479	17829.310973	0.500206
min	0.920000	0.900000	0.850000	0.890000	0.880000	0.860000	0.870000	0.650000	0.650000	0.620000	...	0.870000	0.900000	0.870000	0.880000	0.890000	0.890000	0.860000	0.910000	0.890000	0.000000
25%	19.602500	19.595000	18.925000	19.277500	19.210000	19.582500	18.690000	19.062500	19.532500	19.285000	...	19.197500	18.895000	19.237500	19.385000	19.027500	19.135000	19.205000	18.812500	19.145000	0.000000
50%	301.425000	295.205000	297.260000	299.720000	295.115000	294.380000	295.935000	290.850000	294.565000	295.160000	...	297.845000	295.420000	299.155000	293.355000	301.370000	296.960000	300.925000	299.200000	302.275000	0.500000
75%	5358.795000	5417.847500	5393.367500	5388.482500	5321.987500	5328.040000	5443.977500	5283.655000	5378.180000	5319.097500	...	5355.355000	5386.037500	5286.385000	5345.797500	5300.890000	5361.047500	5390.850000	5288.712500	5357.847500	1.000000
max	117807.870000	108896.480000	119031.350000	110212.590000	113000.470000	116848.390000	115609.240000	118522.320000	112895.900000	117798.300000	...	113858.680000	112948.830000	112409.570000	112933.730000	112037.220000	115110.420000	116431.960000	113291.960000	114533.760000	1.000000
8 rows × 101 columns

df.columns
Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',
       ...
       'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100',
       'Class'],
      dtype='object', length=101)
print(df.columns.tolist())
['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V39', 'V40', 'V41', 'V42', 'V43', 'V44', 'V45', 'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V53', 'V54', 'V55', 'V56', 'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V65', 'V66', 'V67', 'V68', 'V69', 'V70', 'V71', 'V72', 'V73', 'V74', 'V75', 'V76', 'V77', 'V78', 'V79', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V86', 'V87', 'V88', 'V89', 'V90', 'V91', 'V92', 'V93', 'V94', 'V95', 'V96', 'V97', 'V98', 'V99', 'V100', 'Class']
df.shape
(1212, 101)
df['Class'].value_counts()
0    606
1    606
Name: Class, dtype: int64
df.groupby('Class').mean()
V1	V2	V3	V4	V5	V6	V7	V8	V9	V10	...	V91	V92	V93	V94	V95	V96	V97	V98	V99	V100
Class																					
0	7913.333251	7825.339967	7902.497294	7857.032079	7775.610198	7875.436337	7804.166584	7722.324802	7793.328416	7686.782046	...	7753.427244	7737.843366	7799.332079	7825.211700	7791.354010	7927.237112	7874.502343	7844.227459	7875.338713	7855.181172
1	8424.850512	8463.272558	8482.810182	8496.705396	8480.984224	8470.623680	8572.998911	8644.958284	8516.011716	8554.753102	...	8478.513399	8502.270264	8452.502739	8492.375924	8490.416832	8499.724109	8496.685660	8436.163251	8510.583069	8457.213581
2 rows × 100 columns

y = df['Class']
y.shape
(1212,)
y
0       0
1       1
2       1
3       0
4       0
       ..
1207    1
1208    0
1209    1
1210    1
1211    0
Name: Class, Length: 1212, dtype: int64
x = df[['V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V18','V19','V20','V21']]
X = df.drop('Class',axis=1)
X.shape
(1212, 100)
X
V1	V2	V3	V4	V5	V6	V7	V8	V9	V10	...	V91	V92	V93	V94	V95	V96	V97	V98	V99	V100
0	39.02	36.49	38.20	38.85	39.38	39.74	37.02	39.53	38.81	38.79	...	37.57	36.62	36.92	38.80	38.52	38.07	36.73	39.46	37.50	39.10
1	1.83	1.71	1.77	1.77	1.68	1.78	1.80	1.70	1.75	1.78	...	1.71	1.80	1.79	1.77	1.74	1.74	1.80	1.78	1.75	1.69
2	68177.69	66138.42	72981.88	74304.33	67549.66	69367.34	69169.41	73268.61	74465.84	72503.37	...	69384.71	73438.88	71053.35	71112.62	74916.48	72571.58	66348.97	71063.72	67404.27	74920.24
3	44889.06	39191.86	40728.46	38576.36	45876.06	47034.00	46611.43	37668.32	40980.89	38466.15	...	47653.60	42625.67	40684.20	46960.73	44546.80	45410.53	47139.44	43095.68	40888.34	39615.19
4	5.70	5.40	5.28	5.38	5.27	5.61	6.00	5.38	5.34	5.87	...	5.52	5.17	5.67	5.60	5.94	5.73	5.22	5.30	5.73	5.91
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
1207	13.00	12.87	13.27	13.04	13.19	12.53	14.31	13.33	13.63	14.55	...	12.89	12.48	12.15	13.15	12.35	13.58	13.86	12.88	13.87	13.51
1208	48.66	50.11	48.55	50.43	50.09	49.67	48.95	48.65	48.63	48.61	...	47.45	46.93	49.61	47.16	48.17	47.94	49.81	49.89	47.43	47.77
1209	10160.65	9048.63	8994.94	9514.39	9814.74	10195.24	10031.47	10202.28	9152.99	9591.75	...	10413.41	9068.11	9191.80	9275.04	9848.18	9074.17	9601.74	10366.24	8997.60	9305.77
1210	34.81	35.07	34.98	32.37	34.16	34.03	33.31	32.48	35.63	32.48	...	33.18	32.76	35.03	32.89	31.91	33.85	35.28	32.49	32.83	34.82
1211	8489.43	7672.98	9132.14	7985.73	8226.85	8554.28	8838.87	8967.24	8635.14	8544.37	...	7747.70	8609.73	9209.48	8496.33	8724.01	8219.99	8550.86	8679.43	8389.31	8712.80
1212 rows × 100 columns

import matplotlib.pyplot as plt
plt.plot(X.iloc[0,:])
plt.title('Valley');

plt.plot(X.iloc[1,:])
plt.title('Hill');

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
X = ss.fit_transform(X)
X
array([[-0.45248681, -0.45361784, -0.45100881, ..., -0.45609618,
        -0.45164274, -0.45545496],
       [-0.45455665, -0.45556372, -0.45302369, ..., -0.45821768,
        -0.45362255, -0.45755405],
       [ 3.33983504,  3.24466709,  3.58338069, ...,  3.5427869 ,
         3.27907378,  3.74616847],
       ...,
       [ 0.11084204,  0.0505953 ,  0.04437307, ...,  0.12533312,
         0.04456025,  0.06450317],
       [-0.45272112, -0.45369729, -0.45118691, ..., -0.45648861,
        -0.45190136, -0.45569511],
       [ 0.01782872, -0.02636986,  0.05196137, ...,  0.03036056,
         0.01087365,  0.03123129]])
X.shape
(1212, 100)
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X, y,test_size = 0.3,stratify =y,random_state=42529)
X_train.shape,X_test.shape,y_train.shape,y_test.shape
((848, 100), (364, 100), (848,), (364,))
from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train, y_train)
LogisticRegression()
y_pred = lr.predict(X_test)
y_pred.shape
(364,)
y_pred
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,
       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,
       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,
       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1,
       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0,
       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0,
       1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,
       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,
       0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1])
lr.predict_proba(X_test)
array([[0.56336743, 0.43663257],
       [0.50327036, 0.49672964],
       [0.5744651 , 0.4255349 ],
       [0.50737523, 0.49262477],
       [0.50767476, 0.49232524],
       [0.50870658, 0.49129342],
       [0.50793215, 0.49206785],
       [0.60357911, 0.39642089],
       [0.51009653, 0.48990347],
       [0.50964834, 0.49035166],
       [0.5072121 , 0.4927879 ],
       [0.51503417, 0.48496583],
       [0.9359586 , 0.0640414 ],
       [0.5096882 , 0.4903118 ],
       [0.52004957, 0.47995043],
       [0.73731192, 0.26268808],
       [0.4738917 , 0.5261083 ],
       [0.50781845, 0.49218155],
       [0.50862143, 0.49137857],
       [0.50863418, 0.49136582],
       [0.29771937, 0.70228063],
       [0.38273301, 0.61726699],
       [0.50865393, 0.49134607],
       [0.28367971, 0.71632029],
       [0.5087318 , 0.4912682 ],
       [0.50707759, 0.49292241],
       [0.50896134, 0.49103866],
       [0.50811695, 0.49188305],
       [0.50861556, 0.49138444],
       [0.50748418, 0.49251582],
       [0.41565132, 0.58434868],
       [0.51322172, 0.48677828],
       [0.19965036, 0.80034964],
       [0.74863304, 0.25136696],
       [0.5086539 , 0.4913461 ],
       [0.50862562, 0.49137438],
       [0.5086808 , 0.4913192 ],
       [0.50853409, 0.49146591],
       [0.51269829, 0.48730171],
       [0.51582679, 0.48417321],
       [0.50858123, 0.49141877],
       [0.52031809, 0.47968191],
       [0.28012047, 0.71987953],
       [0.51125977, 0.48874023],
       [0.54087675, 0.45912325],
       [0.46730929, 0.53269071],
       [0.50822763, 0.49177237],
       [0.52388229, 0.47611771],
       [0.50104299, 0.49895701],
       [0.5087587 , 0.4912413 ],
       [0.508643  , 0.491357  ],
       [0.54043011, 0.45956989],
       [0.50846683, 0.49153317],
       [0.50733901, 0.49266099],
       [0.51454787, 0.48545213],
       [0.50856523, 0.49143477],
       [0.50860435, 0.49139565],
       [0.50893171, 0.49106829],
       [0.55860167, 0.44139833],
       [0.35542756, 0.64457244],
       [0.50448796, 0.49551204],
       [0.50859469, 0.49140531],
       [0.5086233 , 0.4913767 ],
       [0.33508019, 0.66491981],
       [0.51012543, 0.48987457],
       [0.50853879, 0.49146121],
       [0.56765072, 0.43234928],
       [0.4783306 , 0.5216694 ],
       [0.5086715 , 0.4913285 ],
       [0.50644913, 0.49355087],
       [0.50905598, 0.49094402],
       [0.50862811, 0.49137189],
       [0.51023785, 0.48976215],
       [0.50829137, 0.49170863],
       [0.50959781, 0.49040219],
       [0.5932419 , 0.4067581 ],
       [0.40833642, 0.59166358],
       [0.36255743, 0.63744257],
       [0.77441812, 0.22558188],
       [0.54037266, 0.45962734],
       [0.50859008, 0.49140992],
       [0.5086383 , 0.4913617 ],
       [0.50820559, 0.49179441],
       [0.49643968, 0.50356032],
       [0.50859088, 0.49140912],
       [0.50399119, 0.49600881],
       [0.50865057, 0.49134943],
       [0.50908457, 0.49091543],
       [0.50847805, 0.49152195],
       [0.50056182, 0.49943818],
       [0.64764173, 0.35235827],
       [0.50865272, 0.49134728],
       [0.50962529, 0.49037471],
       [0.52175688, 0.47824312],
       [0.50861731, 0.49138269],
       [0.00188839, 0.99811161],
       [0.45882119, 0.54117881],
       [0.60418363, 0.39581637],
       [0.50866309, 0.49133691],
       [0.53064518, 0.46935482],
       [0.51532303, 0.48467697],
       [0.49165316, 0.50834684],
       [0.54605771, 0.45394229],
       [0.50882287, 0.49117713],
       [0.50537029, 0.49462971],
       [0.55736615, 0.44263385],
       [0.50833558, 0.49166442],
       [0.50016453, 0.49983547],
       [0.50861665, 0.49138335],
       [0.50792355, 0.49207645],
       [0.27493136, 0.72506864],
       [0.50848804, 0.49151196],
       [0.51027349, 0.48972651],
       [0.50866433, 0.49133567],
       [0.55368439, 0.44631561],
       [0.14525858, 0.85474142],
       [0.50812564, 0.49187436],
       [0.50425639, 0.49574361],
       [0.83669288, 0.16330712],
       [0.51807389, 0.48192611],
       [0.5093429 , 0.4906571 ],
       [0.38916499, 0.61083501],
       [0.50855267, 0.49144733],
       [0.48813822, 0.51186178],
       [0.5086503 , 0.4913497 ],
       [0.50869165, 0.49130835],
       [0.50748318, 0.49251682],
       [0.51478611, 0.48521389],
       [0.50957085, 0.49042915],
       [0.56654947, 0.43345053],
       [0.50531203, 0.49468797],
       [0.52184108, 0.47815892],
       [0.50901837, 0.49098163],
       [0.47509918, 0.52490082],
       [0.54368225, 0.45631775],
       [0.50843516, 0.49156484],
       [0.51352796, 0.48647204],
       [0.50857307, 0.49142693],
       [0.48070574, 0.51929426],
       [0.50821317, 0.49178683],
       [0.50859264, 0.49140736],
       [0.61181144, 0.38818856],
       [0.50871786, 0.49128214],
       [0.5096574 , 0.4903426 ],
       [0.50876593, 0.49123407],
       [0.26235723, 0.73764277],
       [0.50862098, 0.49137902],
       [0.71522873, 0.28477127],
       [0.50880258, 0.49119742],
       [0.50865612, 0.49134388],
       [0.50858524, 0.49141476],
       [0.50864686, 0.49135314],
       [0.88654952, 0.11345048],
       [0.50865941, 0.49134059],
       [0.50865122, 0.49134878],
       [0.10671288, 0.89328712],
       [0.50865423, 0.49134577],
       [0.41188851, 0.58811149],
       [0.55572711, 0.44427289],
       [0.51395262, 0.48604738],
       [0.50861693, 0.49138307],
       [0.89581586, 0.10418414],
       [0.50894479, 0.49105521],
       [0.50963738, 0.49036262],
       [0.50845819, 0.49154181],
       [0.62869575, 0.37130425],
       [0.56013679, 0.43986321],
       [0.41050705, 0.58949295],
       [0.51594204, 0.48405796],
       [0.51034501, 0.48965499],
       [0.51006376, 0.48993624],
       [0.50868311, 0.49131689],
       [0.53572344, 0.46427656],
       [0.50889248, 0.49110752],
       [0.57188348, 0.42811652],
       [0.49876231, 0.50123769],
       [0.80087402, 0.19912598],
       [0.50865011, 0.49134989],
       [0.73386094, 0.26613906],
       [0.58443595, 0.41556405],
       [0.50888941, 0.49111059],
       [0.50875191, 0.49124809],
       [0.50856453, 0.49143547],
       [0.99289372, 0.00710628],
       [0.50862516, 0.49137484],
       [0.50902013, 0.49097987],
       [0.42178743, 0.57821257],
       [0.51195923, 0.48804077],
       [0.50693381, 0.49306619],
       [0.49439742, 0.50560258],
       [0.50864167, 0.49135833],
       [0.34064698, 0.65935302],
       [0.50050907, 0.49949093],
       [0.47805583, 0.52194417],
       [0.50862873, 0.49137127],
       [0.50882092, 0.49117908],
       [0.51986748, 0.48013252],
       [0.50876508, 0.49123492],
       [0.52968381, 0.47031619],
       [0.50624452, 0.49375548],
       [0.50866735, 0.49133265],
       [0.52908121, 0.47091879],
       [0.519975  , 0.480025  ],
       [0.29951871, 0.70048129],
       [0.49516999, 0.50483001],
       [0.50677619, 0.49322381],
       [0.50861993, 0.49138007],
       [0.50847961, 0.49152039],
       [0.50858709, 0.49141291],
       [0.57059461, 0.42940539],
       [0.60694761, 0.39305239],
       [0.50848905, 0.49151095],
       [0.45270202, 0.54729798],
       [0.59825748, 0.40174252],
       [0.16132575, 0.83867425],
       [0.5086396 , 0.4913604 ],
       [0.50972406, 0.49027594],
       [0.50865435, 0.49134565],
       [0.44337176, 0.55662824],
       [0.46571328, 0.53428672],
       [0.50860369, 0.49139631],
       [0.50273771, 0.49726229],
       [0.6542158 , 0.3457842 ],
       [0.4883913 , 0.5116087 ],
       [0.5086348 , 0.4913652 ],
       [0.0332069 , 0.9667931 ],
       [0.50858265, 0.49141735],
       [0.5081553 , 0.4918447 ],
       [0.55923852, 0.44076148],
       [0.50893215, 0.49106785],
       [0.07230379, 0.92769621],
       [0.50858319, 0.49141681],
       [0.4224533 , 0.5775467 ],
       [0.42780693, 0.57219307],
       [0.50889481, 0.49110519],
       [0.52570582, 0.47429418],
       [0.49822121, 0.50177879],
       [0.30161898, 0.69838102],
       [0.50821594, 0.49178406],
       [0.50888746, 0.49111254],
       [0.17537278, 0.82462722],
       [0.507106  , 0.492894  ],
       [0.78121491, 0.21878509],
       [0.48397764, 0.51602236],
       [0.50854704, 0.49145296],
       [0.51044178, 0.48955822],
       [0.80138592, 0.19861408],
       [0.50656723, 0.49343277],
       [0.50568626, 0.49431374],
       [0.50864416, 0.49135584],
       [0.50865871, 0.49134129],
       [0.30934703, 0.69065297],
       [0.50848119, 0.49151881],
       [0.05533465, 0.94466535],
       [0.50864406, 0.49135594],
       [0.45699688, 0.54300312],
       [0.5304609 , 0.4695391 ],
       [0.50863726, 0.49136274],
       [0.50861169, 0.49138831],
       [0.50890201, 0.49109799],
       [0.49228275, 0.50771725],
       [0.55054227, 0.44945773],
       [0.49563695, 0.50436305],
       [0.50864184, 0.49135816],
       [0.50547371, 0.49452629],
       [0.50705962, 0.49294038],
       [0.50850823, 0.49149177],
       [0.50863512, 0.49136488],
       [0.60704267, 0.39295733],
       [0.50861906, 0.49138094],
       [0.50836714, 0.49163286],
       [0.50845776, 0.49154224],
       [0.51338133, 0.48661867],
       [0.50859939, 0.49140061],
       [0.5055093 , 0.4944907 ],
       [0.37826991, 0.62173009],
       [0.50859175, 0.49140825],
       [0.91470492, 0.08529508],
       [0.50860394, 0.49139606],
       [0.50846492, 0.49153508],
       [0.5085827 , 0.4914173 ],
       [0.50860135, 0.49139865],
       [0.51559007, 0.48440993],
       [0.50927125, 0.49072875],
       [0.50931327, 0.49068673],
       [0.51131574, 0.48868426],
       [0.50854684, 0.49145316],
       [0.50861359, 0.49138641],
       [0.43520621, 0.56479379],
       [0.50865073, 0.49134927],
       [0.51146007, 0.48853993],
       [0.50818648, 0.49181352],
       [0.46802479, 0.53197521],
       [0.50863674, 0.49136326],
       [0.40298304, 0.59701696],
       [0.52052624, 0.47947376],
       [0.44184753, 0.55815247],
       [0.45694069, 0.54305931],
       [0.34477069, 0.65522931],
       [0.50856493, 0.49143507],
       [0.32961821, 0.67038179],
       [0.5084232 , 0.4915768 ],
       [0.50862837, 0.49137163],
       [0.50861585, 0.49138415],
       [0.49982739, 0.50017261],
       [0.50640414, 0.49359586],
       [0.00939577, 0.99060423],
       [0.50865027, 0.49134973],
       [0.4072756 , 0.5927244 ],
       [0.50783676, 0.49216324],
       [0.50863233, 0.49136767],
       [0.5088199 , 0.4911801 ],
       [0.50626548, 0.49373452],
       [0.48327857, 0.51672143],
       [0.49892381, 0.50107619],
       [0.50862415, 0.49137585],
       [0.5120782 , 0.4879218 ],
       [0.87769411, 0.12230589],
       [0.50885619, 0.49114381],
       [0.50775441, 0.49224559],
       [0.50885086, 0.49114914],
       [0.45953644, 0.54046356],
       [0.50860027, 0.49139973],
       [0.49737446, 0.50262554],
       [0.50902556, 0.49097444],
       [0.03768717, 0.96231283],
       [0.46144805, 0.53855195],
       [0.499096  , 0.500904  ],
       [0.713751  , 0.286249  ],
       [0.50867121, 0.49132879],
       [0.63597658, 0.36402342],
       [0.50894065, 0.49105935],
       [0.5322109 , 0.4677891 ],
       [0.47359178, 0.52640822],
       [0.5113184 , 0.4886816 ],
       [0.47711037, 0.52288963],
       [0.50836866, 0.49163134],
       [0.51791137, 0.48208863],
       [0.50929627, 0.49070373],
       [0.77824421, 0.22175579],
       [0.50878249, 0.49121751],
       [0.53444804, 0.46555196],
       [0.68401995, 0.31598005],
       [0.50837603, 0.49162397],
       [0.49955889, 0.50044111],
       [0.66733462, 0.33266538],
       [0.51287011, 0.48712989],
       [0.51006838, 0.48993162],
       [0.4949386 , 0.5050614 ],
       [0.50415411, 0.49584589],
       [0.50875422, 0.49124578],
       [0.50859966, 0.49140034],
       [0.5171463 , 0.4828537 ],
       [0.49828537, 0.50171463],
       [0.5086824 , 0.4913176 ],
       [0.58878492, 0.41121508],
       [0.50578143, 0.49421857],
       [0.50864402, 0.49135598],
       [0.78691967, 0.21308033],
       [0.50865312, 0.49134688],
       [0.50857367, 0.49142633],
       [0.48168716, 0.51831284],
       [0.50838589, 0.49161411],
       [0.05333436, 0.94666564]])
from sklearn.metrics import confusion_matrix, classification_report
print(confusion_matrix(y_test,y_pred))
[[181   1]
 [106  76]]
print(classification_report(y_test,y_pred))
              precision    recall  f1-score   support

           0       0.63      0.99      0.77       182
           1       0.99      0.42      0.59       182

    accuracy                           0.71       364
   macro avg       0.81      0.71      0.68       364
weighted avg       0.81      0.71      0.68       364

X_new = df.sample(1)
X_new
V1	V2	V3	V4	V5	V6	V7	V8	V9	V10	...	V92	V93	V94	V95	V96	V97	V98	V99	V100	Class
737	17374.47	18911.32	16672.13	17140.79	16758.66	16991.77	19515.99	18120.5	16420.15	19429.54	...	16347.96	16315.1	18096.71	18405.8	18667.32	19745.28	17179.89	18884.24	19742.7	1
1 rows × 101 columns

X_new.shape
(1, 101)
X_new = X_new.drop('Class', axis = 1)
X_new
V1	V2	V3	V4	V5	V6	V7	V8	V9	V10	...	V91	V92	V93	V94	V95	V96	V97	V98	V99	V100
737	17374.47	18911.32	16672.13	17140.79	16758.66	16991.77	19515.99	18120.5	16420.15	19429.54	...	19827.73	16347.96	16315.1	18096.71	18405.8	18667.32	19745.28	17179.89	18884.24	19742.7
1 rows × 100 columns

X_new.shape
(1, 100)
X_new =ss.fit_transform(X_new)
y_pred_new = lr.predict(X_new)
y_pred_new
array([1])
lr.predict_proba(X_new)
array([[0.49714991, 0.50285009]])